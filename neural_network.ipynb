{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ],
   "execution_count":1,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"3JFVX85ZlmUhht4dOGU0Ia",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class VolumePredictor:\n",
    "    def __init__(self, stock, neurons_layer, lags, frequency, direction, validation_split=0.25):\n",
    "        self.stock = stock\n",
    "        self.neurons_layer = neurons_layer\n",
    "        self.lags = lags\n",
    "        self.frequency = frequency\n",
    "        self.direction = direction\n",
    "        self.validation_split = validation_split\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        if self.direction == 'total':\n",
    "            self.volume = 'volume'\n",
    "        elif self.direction == 'buy':\n",
    "            self.volume = 'buyVol'\n",
    "        else:\n",
    "            self.volume = 'sellVol'\n",
    "        \n",
    "    def load_data(self, file_path):\n",
    "        table = pq.read_table(file_path)\n",
    "        df = table.to_pandas()\n",
    "        df['ts_event'] = pd.to_datetime(df['ts_event'])\n",
    "        df.set_index('ts_event', inplace=True)\n",
    "\n",
    "        # Start from 2024\/09\n",
    "        df = df[(df.index >= '2024\/09') & (df.index < '2025\/02')]\n",
    "        if self.frequency == '15min':\n",
    "            df = df.resample('15T').sum(numeric_only=True)\n",
    "        df = df[(df.index.time >= pd.to_datetime('09:30').time()) & \n",
    "                (df.index.time < pd.to_datetime('16:00').time())]\n",
    "        \n",
    "        df = df[[self.volume]]\n",
    "        df = df[df[self.volume] != 0]\n",
    "\n",
    "        # Verify data structure\n",
    "        if self.volume not in df.columns:\n",
    "            volume_col = df.columns[0]\n",
    "            df = df.rename(columns={volume_col: self.volume})\n",
    "            print(f\"Renamed column '{volume_col}' to '{self.volume}'\")\n",
    "\n",
    "        # Convert to numpy array and handle missing values\n",
    "        volume_data = df[[self.volume]].fillna(method='ffill')\n",
    "\n",
    "        return volume_data\n",
    "            \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"\n",
    "        Prepare data for the NAR model with specified number of lags and validation split\n",
    "        \"\"\"\n",
    "        # Scale the data\n",
    "        scaled_data = self.scaler.fit_transform(data[self.volume].values.reshape(-1, 1))\n",
    "        \n",
    "        # Create sequences with dates\n",
    "        X, y, dates = [], [], []\n",
    "        for i in range(len(scaled_data) - self.lags):\n",
    "            X.append(scaled_data[i:(i + self.lags), 0])\n",
    "            y.append(scaled_data[i + self.lags, 0])\n",
    "            dates.append(data.index[i + self.lags])\n",
    "            \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        dates = pd.DatetimeIndex(dates)\n",
    "\n",
    "        # Split into train, validation, and test sets\n",
    "        test_mask = dates >= '2025\/01'\n",
    "        train_val_mask = ~test_mask\n",
    "        \n",
    "        X_train_val = X[train_val_mask]\n",
    "        y_train_val = y[train_val_mask]\n",
    "        train_val_dates = dates[train_val_mask]\n",
    "        \n",
    "        # Further split training data into train and validation\n",
    "        val_size = int(len(X_train_val) * self.validation_split)\n",
    "        \n",
    "        X_train = X_train_val[:-val_size]\n",
    "        y_train = y_train_val[:-val_size]\n",
    "        train_dates = train_val_dates[:-val_size]\n",
    "        \n",
    "        X_val = X_train_val[-val_size:]\n",
    "        y_val = y_train_val[-val_size:]\n",
    "        val_dates = train_val_dates[-val_size:]\n",
    "        \n",
    "        X_test = X[test_mask]\n",
    "        y_test = y[test_mask]\n",
    "        test_dates = dates[test_mask]\n",
    "        \n",
    "        return (X_train, y_train, train_dates), (X_val, y_val, val_dates), (X_test, y_test, test_dates)\n",
    "\n",
    "    def build_model(self):\n",
    "        if len(self.neurons_layer) == 1:\n",
    "            # Build an autoregressive neural network with one hidden layer\n",
    "            self.model = Sequential(\n",
    "                [\n",
    "                Dense(self.neurons_layer[0], activation='sigmoid', input_dim=self.lags),\n",
    "                Dense(1, activation='linear')\n",
    "            ]\n",
    "            )\n",
    "        else:\n",
    "            # Build an autoregressive neural network with two hidden layers\n",
    "            self.model = Sequential([\n",
    "                Dense(self.neurons_layer[0],\n",
    "                    activation='relu', \n",
    "                    input_dim=self.lags,\n",
    "                    kernel_initializer='he_normal'), \n",
    "                Dense(self.neurons_layer[1], \n",
    "                    activation='relu',\n",
    "                    kernel_initializer='he_normal'),\n",
    "                Dense(1, activation='linear')\n",
    "            ])\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse'\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the model with validation and early stopping\n",
    "        \"\"\"\n",
    "        # Define callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        self.plot_loss_curves()\n",
    "        self.plot_loss_relationship()\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def plot_loss_curves(self):\n",
    "        \"\"\"\n",
    "        Plot training and validation loss curves\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Model Loss (Neurons: {self.neurons_layer if len(self.neurons_layer) > 1 else str(self.neurons_layer[0])}, Lags: {self.lags})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'\/data\/workspace_files\/image\/{self.direction}\/{self.stock}_hn_{self.neurons_layer}_l_{self.lags}_{self.frequency}_model_loss_{self.direction}.jpg')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_relationship(self):\n",
    "        \"\"\"\n",
    "        Plot the relationship between training and validation loss\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "            \n",
    "        train_losses = self.history.history['loss']\n",
    "        val_losses = self.history.history['val_loss']\n",
    "        \n",
    "        # Find minimum validation loss and its corresponding training loss\n",
    "        min_val_loss_idx = np.argmin(val_losses)\n",
    "        min_val_loss = val_losses[min_val_loss_idx]\n",
    "        corresponding_train_loss = train_losses[min_val_loss_idx]\n",
    "        \n",
    "        plt.figure(figsize=(5, 3))\n",
    "        \n",
    "        # Set up the plot area\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xlabel('L_train')\n",
    "        plt.ylabel('L_val')\n",
    "        plt.plot(train_losses, val_losses, 'b-', alpha=0.5)\n",
    "        \n",
    "        # Add arrow for optimal point\n",
    "        plt.plot([corresponding_train_loss], [min_val_loss], 'ko', \n",
    "                label='min loss function, lack of generalization')\n",
    "        \n",
    "        # Add triangle for overfitting point\n",
    "        if len(train_losses) > min_val_loss_idx + 1:\n",
    "            overfitting_idx = min_val_loss_idx + 1\n",
    "            plt.plot([train_losses[overfitting_idx]], [val_losses[overfitting_idx]], \n",
    "                    '^k', label='overfitting')\n",
    "        \n",
    "        # Set axis limits with some padding\n",
    "        min_train = min(train_losses)\n",
    "        max_train = max(train_losses)\n",
    "        min_val = min(val_losses)\n",
    "        max_val = max(val_losses)\n",
    "        \n",
    "        plt.xlim(min_train * 0.9, max_train * 1.1)\n",
    "        plt.ylim(min_val * 0.9, max_val * 1.1)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title('plot_loss_relationship')\n",
    "        plt.savefig(f'\/data\/workspace_files\/image\/{self.direction}\/{self.stock}_hn_{self.neurons_layer}_l_{self.lags}_{self.frequency}_loss_relationship_{self.direction}.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print the optimal point information\n",
    "        print(f\"Optimal point:\")\n",
    "        print(f\"Train Loss: {corresponding_train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {min_val_loss:.4f}\")\n",
    "        print(f\"Reach at the {min_val_loss_idx + 1} th round\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions and inverse transform to original scale\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(X)\n",
    "        return self.scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate both RRMSE and MAE metrics\n",
    "        \"\"\"\n",
    "        y_true = self.scaler.inverse_transform(y_true.reshape(-1, 1))\n",
    "        y_pred = self.scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "        \n",
    "        rrmse = np.sqrt(np.mean(np.square((y_true - y_pred))))\/ np.mean(y_true)\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        \n",
    "        return {'rrmse': rrmse, 'mae': mae}\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \"\"\"\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load a previously trained model\n",
    "        \"\"\"\n",
    "        from tensorflow.keras.models import load_model\n",
    "        self.model = load_model(filepath)"
   ],
   "execution_count":2,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"YZXlMMVI7SjDXEg3yK8dmh",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Grid search\n",
    "for d in ['total', 'buy', 'sell']:\n",
    "    for f in ['1min', '15min']:\n",
    "        for s in ['SPY', 'QQQ']:\n",
    "            results = pd.DataFrame(columns= ['Stock','Hidden Neurons', 'Lags', \n",
    "                                             'Training RRMSE',  'Validation RRMSE', 'Testing RRMSE',\n",
    "                                             ])\n",
    "            for hn in [[5], [10], [20], [40], [2,2],[5,2],[5,5],[16,8], [32,16]]:\n",
    "                for l in [2, 3, 5, 10, 20, 30, 50]:\n",
    "                    # Initialize predictor\n",
    "                    predictor = VolumePredictor(stock=s, neurons_layer=hn, lags=l, frequency=f, direction = d)\n",
    "\n",
    "                    # Load data from parquet file\n",
    "                    volume_data = predictor.load_data(f'\/data\/workspace_files\/pq\/{s.lower()}_bar_1m_tradeDir.pq')\n",
    "\n",
    "                    (X_train, y_train, train_dates), (X_val, y_val, val_dates), (X_test, y_test, test_dates) = predictor.prepare_data(volume_data)\n",
    "\n",
    "                    # Build and train model\n",
    "                    predictor.build_model()\n",
    "                    history = predictor.train(X_train, y_train, X_val, y_val, epochs=200, batch_size=32)\n",
    "\n",
    "                    # Make predictions\n",
    "                    train_pred = predictor.model.predict(X_train)\n",
    "                    val_pred = predictor.model.predict(X_val)\n",
    "                    test_pred = predictor.model.predict(X_test)\n",
    "                    test_pred = np.maximum(test_pred, 0)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    train_loss = predictor.calculate_metrics(y_train, train_pred)\n",
    "                    test_loss = predictor.calculate_metrics(y_test, test_pred)\n",
    "                    val_loss = predictor.calculate_metrics(y_val, val_pred)\n",
    "\n",
    "\n",
    "                    y_test_orig = predictor.scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "                    test_pred_orig = predictor.scaler.inverse_transform(test_pred.reshape(-1, 1))\n",
    "\n",
    "                    # Create DataFrame for this model's predictions\n",
    "                    model_predictions = pd.DataFrame({\n",
    "                        'Date': test_dates,\n",
    "                        'Actual': y_test_orig.flatten(),\n",
    "                        'Predicted': test_pred_orig.flatten(),\n",
    "                        'Hidden_Neurons': f\"{hn[0]},{hn[1]}\" if len(hn) > 1 else hn[0],\n",
    "                        'Lags': l\n",
    "                    })\n",
    "\n",
    "                    # Plot results\n",
    "                    plt.figure(figsize=(5, 3))\n",
    "                    plt.plot(test_dates, y_test_orig, label='Actual')\n",
    "                    plt.plot(test_dates, test_pred_orig, label='Predicted')\n",
    "                    plt.title(f'Test Set: Actual vs Predicted for {hn if len(hn) > 1 else str(hn[0])} neurons with lag {l}')\n",
    "                    plt.xlabel('Time')\n",
    "                    plt.ylabel('Trading Volume')\n",
    "                    plt.legend()\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f'\/data\/workspace_files\/image\/{d}\/{s}_hn_{hn}_l_{l}_{f}_volume_prediction_{d}.jpg')\n",
    "                    plt.show()\n",
    "                    results.loc[len(results)] = [s, hn, l,\n",
    "                                                f'{train_loss[\"rrmse\"]:.3f}', f'{val_loss[\"rrmse\"]:.3f}', f'{test_loss[\"rrmse\"]:.3f}'\n",
    "                                                ]  \n",
    "\n",
    "                    model_predictions.to_csv(f'\/data\/workspace_files\/20250225\/result\/{d}\/{s}_hn_{hn if len(hn) > 1 else str(hn[0])}_l_{l}_{f}_{d}.csv')\n",
    "\n",
    "                    \n",
    "            results.to_csv(f'\/data\/workspace_files\/result\/{s}_model_performance_summary_{f}_{d}.csv')"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"3umOFbrysw2r6nSDtcXI8h",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[],
   "report_row_ids":[],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}